{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salsa\\PycharmProjects\\endpoint\\env-endpoint\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Salsa\\PycharmProjects\\endpoint\\env-endpoint\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, LayerNormalization, MultiHeadAttention, \n",
    "    Reshape, Flatten, Concatenate, Activation, Add, Input\n",
    ")\n",
    "from tensorflow.keras.models import Model  \n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.activations import gelu\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import fitz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessorWithPyMuPDF:\n",
    "    def __init__(self, max_length=1024):\n",
    "        self.model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        # Correctly initialize the sentence transformer\n",
    "        self.sentence_transformer = SentenceTransformer(self.model_name)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def extract_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF using PyMuPDF\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text() + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "        return text\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\-\\'àáâãäåèéêëìíîïòóôõöùúûüýÿÀÁÂÃÄÅÈÉÊËÌÍÎÏÒÓÔÕÖÙÚÛÜÝ]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def hapus_duplikat(self, text: str) -> str:\n",
    "        seen = set()\n",
    "        result_text = []\n",
    "        lines = text.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line and cleaned_line not in seen:\n",
    "                seen.add(cleaned_line)\n",
    "                result_text.append(cleaned_line)\n",
    "        return \"\\n\".join(result_text)\n",
    "    \n",
    "    def cut_isi(self, text: str) -> str:\n",
    "        pola_2dapus = re.compile(r'(?<=daftar pustaka)(.*?)(?=daftar pustaka)', re.IGNORECASE)\n",
    "        match = pola_2dapus.search(text)\n",
    "        if match:\n",
    "            hasil = match.group(1)\n",
    "            return hasil\n",
    "        else:\n",
    "            pola_1dapus = re.compile(r'(.*?)(daftar pustaka)', re.IGNORECASE)\n",
    "            cek_1dapus = pola_1dapus.search(text)\n",
    "            if cek_1dapus:\n",
    "                hasil = cek_1dapus.group(1)\n",
    "                return hasil\n",
    "            else:\n",
    "                return \"Tidak ditemukan kata 'daftar pustaka' sama sekali.\"\n",
    "\n",
    "    def cut_daftar(self, text: str) -> str:\n",
    "        pola_titik = re.compile(r'\\.{10,}', re.DOTALL)\n",
    "        matches = list(pola_titik.finditer(text))\n",
    "        if matches:\n",
    "            last_match = matches[-1]\n",
    "            last_match_end = last_match.end()\n",
    "            text = text[last_match_end:].strip()\n",
    "        return text\n",
    "    \n",
    "    def generate_embeddings(self, texts, batch_size=32):\n",
    "        \"\"\"Generate embeddings for texts\"\"\"\n",
    "        embeddings_list = []\n",
    "        # Ensure texts is a list\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            # Use encode method from sentence_transformer\n",
    "            batch_embeddings = self.sentence_transformer.encode(batch, convert_to_tensor=True)\n",
    "            embeddings_list.append(batch_embeddings)\n",
    "        \n",
    "        return torch.cat(embeddings_list, dim=0)\n",
    " \n",
    "    def augment_text_methods(self, text: str) -> List[str]:\n",
    "        \"\"\"Text augmentation methods\"\"\"\n",
    "        augmented = []\n",
    "        \n",
    "        # 1. Sentence shuffling\n",
    "        sentences = text.split('. ')\n",
    "        if len(sentences) > 1:\n",
    "            shuffled = sentences.copy()\n",
    "            np.random.shuffle(shuffled)\n",
    "            augmented.append('. '.join(shuffled))\n",
    "        \n",
    "        # 2. Random deletion\n",
    "        words = text.split()\n",
    "        if len(words) > 10:\n",
    "            n_to_delete = max(1, int(len(words) * 0.1))\n",
    "            keep_indices = np.random.choice(\n",
    "                len(words), \n",
    "                len(words) - n_to_delete, \n",
    "                replace=False\n",
    "            )\n",
    "            augmented.append(' '.join([words[i] for i in sorted(keep_indices)]))\n",
    "            \n",
    "        # 3. Random insertion\n",
    "        if len(words) > 5:\n",
    "            n_to_insert = max(1, int(len(words) * 0.1))\n",
    "            augmented_words = words.copy()\n",
    "            for _ in range(n_to_insert):\n",
    "                pos = np.random.randint(0, len(words))\n",
    "                word = np.random.choice(words)\n",
    "                augmented_words.insert(pos, word)\n",
    "            augmented.append(' '.join(augmented_words))\n",
    "                \n",
    "        return augmented\n",
    "\n",
    "    def create_augmented_dataset(self, texts: List[str], n_augment: int = 2) -> List[str]:\n",
    "        \"\"\"Create augmented dataset\"\"\"\n",
    "        augmented_texts = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Keep original\n",
    "            augmented_texts.append(text)\n",
    "            \n",
    "            # Add augmentations\n",
    "            for _ in range(n_augment):\n",
    "                augmented = self.augment_text_methods(text)\n",
    "                if augmented:\n",
    "                    augmented_texts.extend(augmented)\n",
    "                    \n",
    "        return augmented_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeynMindModelV3:\n",
    "    def __init__(self, embedding_dim=384, dropout_rate=0.3, l2_regularization=1e-4):\n",
    "        # Input layers\n",
    "        text_input = Input(shape=(embedding_dim // 2,), name='text_input')\n",
    "        explanation_input = Input(shape=(embedding_dim // 2,), name='explanation_input')\n",
    "        \n",
    "        # Initial normalization with improved epsilon\n",
    "        text_norm = LayerNormalization(epsilon=1e-6)(text_input)\n",
    "        explanation_norm = LayerNormalization(epsilon=1e-6)(explanation_input)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        text_reshaped = Reshape((1, embedding_dim // 2))(text_norm)\n",
    "        explanation_reshaped = Reshape((1, embedding_dim // 2))(explanation_norm)\n",
    "        \n",
    "        # Self-attention blocks\n",
    "        text_attention = MultiHeadAttention(num_heads=4, key_dim=embedding_dim // 8)(\n",
    "            text_reshaped, text_reshaped\n",
    "        )\n",
    "        explanation_attention = MultiHeadAttention(num_heads=4, key_dim=embedding_dim // 8)(\n",
    "            explanation_reshaped, explanation_reshaped\n",
    "        )\n",
    "        \n",
    "        # Flatten attention outputs\n",
    "        text_flat = Flatten()(text_attention)\n",
    "        explanation_flat = Flatten()(explanation_attention)\n",
    "        \n",
    "        # Merge inputs with attention outputs\n",
    "        x = Concatenate()([text_flat, explanation_flat])\n",
    "        \n",
    "        # First dense block with improved residual\n",
    "        residual = x\n",
    "        \n",
    "        # Wider networks often perform better\n",
    "        x = Dense(768, kernel_regularizer=tf.keras.regularizers.l2(l2_regularization))(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = gelu(x)  # Using gelu directly instead of Activation layer\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        \n",
    "        x = Dense(768, kernel_regularizer=tf.keras.regularizers.l2(l2_regularization))(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = gelu(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        \n",
    "        # Residual connection with projection\n",
    "        residual = Dense(768)(residual)\n",
    "        x = Add()([x, residual])\n",
    "        \n",
    "        # Second dense block with gradual dimension reduction\n",
    "        x = Dense(384, kernel_regularizer=tf.keras.regularizers.l2(l2_regularization))(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = gelu(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        \n",
    "        # Task-specific heads with deeper architecture\n",
    "        # Understanding branch\n",
    "        understanding = Dense(256, activation='gelu')(x)\n",
    "        understanding = LayerNormalization(epsilon=1e-6)(understanding)\n",
    "        understanding = Dropout(0.2)(understanding)\n",
    "        understanding = Dense(128, activation='gelu')(understanding)\n",
    "        understanding = LayerNormalization(epsilon=1e-6)(understanding)\n",
    "        understanding = Dropout(0.1)(understanding)\n",
    "        understanding_output = Dense(3, activation='softmax', name='understanding')(understanding)\n",
    "        \n",
    "        # Completeness branch\n",
    "        completeness = Dense(256, activation='gelu')(x)\n",
    "        completeness = LayerNormalization(epsilon=1e-6)(completeness)\n",
    "        completeness = Dropout(0.2)(completeness)\n",
    "        completeness = Dense(128, activation='gelu')(completeness)\n",
    "        completeness = LayerNormalization(epsilon=1e-6)(completeness)\n",
    "        completeness = Dropout(0.1)(completeness)\n",
    "        completeness_output = Dense(3, activation='softmax', name='completeness')(completeness)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = Model(\n",
    "            inputs=[text_input, explanation_input],\n",
    "            outputs=[understanding_output, completeness_output]\n",
    "        )\n",
    "\n",
    "    def compile_model(self, learning_rate=0.002):\n",
    "        \"\"\"Custom compilation with improved focal loss and label smoothing\"\"\"\n",
    "        def focal_loss_with_smoothing(gamma=2.0, alpha=0.25, smoothing=0.1):\n",
    "            def loss_fn(y_true, y_pred):\n",
    "                epsilon = tf.keras.backend.epsilon()\n",
    "                y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "                \n",
    "                # Apply label smoothing\n",
    "                n_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n",
    "                y_true = y_true * (1.0 - smoothing) + (smoothing / n_classes)\n",
    "                \n",
    "                # Calculate focal loss\n",
    "                ce = -y_true * tf.math.log(y_pred)\n",
    "                weight = tf.pow(1 - y_pred, gamma) * y_true\n",
    "                fl = alpha * weight * ce\n",
    "                \n",
    "                return tf.reduce_sum(fl, axis=-1)\n",
    "            return loss_fn\n",
    "        \n",
    "        # Use AdamW with improved parameters\n",
    "        optimizer = AdamW(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=1e-5,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-8,\n",
    "            amsgrad=True\n",
    "        )\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss={\n",
    "                'understanding': focal_loss_with_smoothing(gamma=2.0, alpha=0.25, smoothing=0.1),\n",
    "                'completeness': focal_loss_with_smoothing(gamma=2.0, alpha=0.25, smoothing=0.1)\n",
    "            },\n",
    "            metrics={\n",
    "                'understanding': ['accuracy', Precision(name='precision'), Recall(name='recall')],\n",
    "                'completeness': ['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_pdf_data(pdf_dir: str, augment: bool = True, n_augment: int = 2):\n",
    "    \"\"\"Load and preprocess PDF documents\"\"\"\n",
    "    processor = TextProcessorWithPyMuPDF()\n",
    "    all_texts = []\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(pdf_dir):\n",
    "        raise ValueError(f\"Directory {pdf_dir} does not exist\")\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
    "    \n",
    "    # Check if there are any PDF files\n",
    "    if not pdf_files:\n",
    "        raise ValueError(f\"No PDF files found in {pdf_dir}\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        try:\n",
    "            raw_text = processor.extract_from_pdf(pdf_path)\n",
    "            if raw_text:\n",
    "                cleaned_text = processor.clean_text(raw_text)\n",
    "                all_texts.append(cleaned_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully processed {len(all_texts)} documents\")\n",
    "    \n",
    "    # Check if any texts were processed\n",
    "    if not all_texts:\n",
    "        raise ValueError(\"No text could be extracted from the PDFs\")\n",
    "    \n",
    "    if augment:\n",
    "        all_texts = processor.create_augmented_dataset(all_texts, n_augment)\n",
    "        \n",
    "    print(f\"Total texts after augmentation: {len(all_texts)}\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = processor.generate_embeddings(all_texts)\n",
    "    embeddings_np = embeddings.cpu().numpy() if torch.cuda.is_available() else embeddings.numpy()\n",
    "    \n",
    "    # Split embeddings\n",
    "    half_dim = embeddings_np.shape[1] // 2\n",
    "    input_data = {\n",
    "        \"text_input\": embeddings_np[:, :half_dim],\n",
    "        \"explanation_input\": embeddings_np[:, half_dim:]\n",
    "    }\n",
    "    \n",
    "    # Generate balanced labels\n",
    "    n_samples = len(all_texts)\n",
    "    labels = np.random.randint(0, 3, n_samples)\n",
    "    \n",
    "    # Balance classes\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    balanced_indices = []\n",
    "    for i in range(3):\n",
    "        indices = np.where(labels == i)[0]\n",
    "        if len(indices) > 0:\n",
    "            balanced_indices.extend(\n",
    "                resample(indices, n_samples=max(len(indices), n_samples//3))\n",
    "            )\n",
    "    \n",
    "    # Convert to categorical\n",
    "    output_data = {\n",
    "        \"understanding\": to_categorical(labels[balanced_indices], num_classes=3),\n",
    "        \"completeness\": to_categorical(np.random.randint(0, 3, len(balanced_indices)), num_classes=3)\n",
    "    }\n",
    "    \n",
    "    # Update input data\n",
    "    for key in input_data:\n",
    "        input_data[key] = input_data[key][balanced_indices]\n",
    "        \n",
    "    return input_data, output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y):\n",
    "    \"\"\"\n",
    "    Compute balanced class weights for training\n",
    "    \n",
    "    Args:\n",
    "        y: One-hot encoded labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping class indices to weights\n",
    "    \"\"\"\n",
    "    # Convert one-hot to class indices\n",
    "    y_indices = np.argmax(y, axis=1)\n",
    "    \n",
    "    # Get class counts\n",
    "    classes = np.unique(y_indices)\n",
    "    counts = np.bincount(y_indices)\n",
    "    \n",
    "    # Calculate weights\n",
    "    n_samples = len(y_indices)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    weights = {}\n",
    "    for i in range(n_classes):\n",
    "        weights[i] = n_samples / (n_classes * counts[i])\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_v3(pdf_dir):\n",
    "    # Load data\n",
    "    input_data, output_data = load_and_preprocess_pdf_data(\n",
    "        pdf_dir,\n",
    "        augment=True,\n",
    "        n_augment=2\n",
    "    )\n",
    "    \n",
    "    # Split data with improved stratification\n",
    "    combined_labels = np.argmax(output_data['understanding'], axis=1) * 3 + np.argmax(output_data['completeness'], axis=1)\n",
    "    \n",
    "    train_idx, val_idx = train_test_split(\n",
    "        np.arange(input_data['text_input'].shape[0]), \n",
    "        test_size=0.15,\n",
    "        stratify=combined_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Prepare training and validation sets\n",
    "    X_train = {\n",
    "        'text_input': input_data['text_input'][train_idx],\n",
    "        'explanation_input': input_data['explanation_input'][train_idx]\n",
    "    }\n",
    "    y_train = {\n",
    "        'understanding': output_data['understanding'][train_idx],\n",
    "        'completeness': output_data['completeness'][train_idx]\n",
    "    }\n",
    "    \n",
    "    X_val = {\n",
    "        'text_input': input_data['text_input'][val_idx],\n",
    "        'explanation_input': input_data['explanation_input'][val_idx]\n",
    "    }\n",
    "    y_val = {\n",
    "        'understanding': output_data['understanding'][val_idx],\n",
    "        'completeness': output_data['completeness'][val_idx]\n",
    "    }\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FeynMindModelV3(dropout_rate=0.3, l2_regularization=1e-4)\n",
    "    \n",
    "    # Modified loss functions to handle class weights internally\n",
    "    def weighted_focal_loss(gamma=2.0, alpha=0.25, smoothing=0.1):\n",
    "        def loss_fn(y_true, y_pred):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "            \n",
    "            # Apply label smoothing\n",
    "            n_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n",
    "            y_true = y_true * (1.0 - smoothing) + (smoothing / n_classes)\n",
    "            \n",
    "            # Calculate sample weights based on class distribution\n",
    "            class_counts = tf.reduce_sum(y_true, axis=0)\n",
    "            total_samples = tf.reduce_sum(class_counts)\n",
    "            class_weights = total_samples / (n_classes * class_counts + epsilon)\n",
    "            sample_weights = tf.reduce_sum(y_true * class_weights, axis=1)\n",
    "            \n",
    "            # Calculate focal loss\n",
    "            ce = -y_true * tf.math.log(y_pred)\n",
    "            weight = tf.pow(1 - y_pred, gamma) * y_true\n",
    "            fl = alpha * weight * ce * tf.expand_dims(sample_weights, -1)\n",
    "            \n",
    "            return tf.reduce_sum(fl, axis=-1)\n",
    "        return loss_fn\n",
    "    \n",
    "    # Define initial learning rate\n",
    "    initial_learning_rate = 0.002\n",
    "    \n",
    "    # Custom learning rate schedule with warmup\n",
    "    def lr_schedule(epoch):\n",
    "        if epoch < 10:  # Warmup period\n",
    "            return initial_learning_rate * min(1., epoch / 10.)\n",
    "        return initial_learning_rate * 0.7 ** (epoch // 7)  # Decay after warmup\n",
    "    \n",
    "    # Compile with modified loss functions\n",
    "    optimizer = AdamW(\n",
    "        learning_rate=initial_learning_rate,\n",
    "        weight_decay=1e-5,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8,\n",
    "        amsgrad=True\n",
    "    )\n",
    "    \n",
    "    model.model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss={\n",
    "            'understanding': weighted_focal_loss(gamma=2.0, alpha=0.25, smoothing=0.1),\n",
    "            'completeness': weighted_focal_loss(gamma=2.0, alpha=0.25, smoothing=0.1)\n",
    "        },\n",
    "        metrics={\n",
    "            'understanding': ['accuracy', Precision(name='precision'), Recall(name='recall')],\n",
    "            'completeness': ['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Enhanced callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=25,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-4\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.7,\n",
    "            patience=7,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'feynmind_model_v3.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False\n",
    "        ),\n",
    "        LearningRateScheduler(lr_schedule)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate class weights for balancing\n",
    "# class_weights = class_weight.compute_class_weight(\n",
    "#     class_weight='balanced',\n",
    "#     classes=np.unique(np.argmax(y_train['understanding'], axis=1)),\n",
    "#     y=np.argmax(y_train['understanding'], axis=1)\n",
    "# )\n",
    "# class_weights = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# # Create data pipelines with augmentation and class balancing\n",
    "# train_data = create_data_pipeline(X_train, y_train, batch_size=16, augment=True, num_augmented=1)\n",
    "# val_data = create_data_pipeline(X_val, y_val, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    pdf_dir = \"Perkategori/\"\n",
    "    model, history = train_model_v3(pdf_dir)\n",
    "    \n",
    "    # Save model\n",
    "    # model.model.save('similarity_model_2.h5')\n",
    "    model.model.export(\"saved_model\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['understanding_accuracy'], label='Understanding (Train)')\n",
    "    plt.plot(history.history['val_understanding_accuracy'], label='Understanding (Val)')\n",
    "    plt.title('Understanding Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['completeness_accuracy'], label='Completeness (Train)')\n",
    "    plt.plot(history.history['val_completeness_accuracy'], label='Completeness (Val)')\n",
    "    plt.title('Completeness Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 38 documents\n",
      "Total texts after augmentation: 190\n",
      "Epoch 1/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 317ms/step - completeness_accuracy: 0.2840 - completeness_loss: 0.2473 - completeness_precision: 0.2911 - completeness_recall: 0.2213 - loss: 0.7068 - understanding_accuracy: 0.3692 - understanding_loss: 0.2871 - understanding_precision: 0.4092 - understanding_recall: 0.3501 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.2516 - val_completeness_precision: 0.3571 - val_completeness_recall: 0.3448 - val_loss: 0.9890 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.5583 - val_understanding_precision: 0.3448 - val_understanding_recall: 0.3448 - learning_rate: 0.0000e+00\n",
      "Epoch 2/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - completeness_accuracy: 0.3431 - completeness_loss: 0.2259 - completeness_precision: 0.3659 - completeness_recall: 0.2880 - loss: 0.6932 - understanding_accuracy: 0.3269 - understanding_loss: 0.2913 - understanding_precision: 0.3177 - understanding_recall: 0.2801 - val_completeness_accuracy: 0.4483 - val_completeness_loss: 0.1097 - val_completeness_precision: 0.3333 - val_completeness_recall: 0.1034 - val_loss: 0.4036 - val_understanding_accuracy: 0.4138 - val_understanding_loss: 0.1149 - val_understanding_precision: 0.3333 - val_understanding_recall: 0.0690 - learning_rate: 2.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - completeness_accuracy: 0.3425 - completeness_loss: 0.1921 - completeness_precision: 0.3081 - completeness_recall: 0.2250 - loss: 0.6202 - understanding_accuracy: 0.2882 - understanding_loss: 0.2443 - understanding_precision: 0.3040 - understanding_recall: 0.2469 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1499 - val_completeness_precision: 0.2692 - val_completeness_recall: 0.2414 - val_loss: 0.4607 - val_understanding_accuracy: 0.2759 - val_understanding_loss: 0.1318 - val_understanding_precision: 0.3333 - val_understanding_recall: 0.2069 - learning_rate: 4.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - completeness_accuracy: 0.3488 - completeness_loss: 0.1778 - completeness_precision: 0.3140 - completeness_recall: 0.2248 - loss: 0.5785 - understanding_accuracy: 0.4170 - understanding_loss: 0.2137 - understanding_precision: 0.3881 - understanding_recall: 0.2747 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1305 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4506 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1411 - val_understanding_precision: 0.3889 - val_understanding_recall: 0.2414 - learning_rate: 6.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - completeness_accuracy: 0.3005 - completeness_loss: 0.2194 - completeness_precision: 0.3029 - completeness_recall: 0.2280 - loss: 0.5916 - understanding_accuracy: 0.2862 - understanding_loss: 0.2007 - understanding_precision: 0.3470 - understanding_recall: 0.2246 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1263 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4677 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1624 - val_understanding_precision: 0.3448 - val_understanding_recall: 0.3448 - learning_rate: 8.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - completeness_accuracy: 0.3580 - completeness_loss: 0.1638 - completeness_precision: 0.2731 - completeness_recall: 0.1526 - loss: 0.5373 - understanding_accuracy: 0.3473 - understanding_loss: 0.1896 - understanding_precision: 0.3568 - understanding_recall: 0.2645 - val_completeness_accuracy: 0.3103 - val_completeness_loss: 0.1132 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4342 - val_understanding_accuracy: 0.3103 - val_understanding_loss: 0.1421 - val_understanding_precision: 0.3103 - val_understanding_recall: 0.3103 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - completeness_accuracy: 0.3593 - completeness_loss: 0.1576 - completeness_precision: 0.3019 - completeness_recall: 0.1621 - loss: 0.5050 - understanding_accuracy: 0.3137 - understanding_loss: 0.1620 - understanding_precision: 0.3016 - understanding_recall: 0.1879 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1162 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4255 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1305 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 0.0012\n",
      "Epoch 8/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - completeness_accuracy: 0.3544 - completeness_loss: 0.1584 - completeness_precision: 0.2741 - completeness_recall: 0.1431 - loss: 0.4953 - understanding_accuracy: 0.3711 - understanding_loss: 0.1565 - understanding_precision: 0.4035 - understanding_recall: 0.2368 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1068 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4040 - val_understanding_accuracy: 0.3103 - val_understanding_loss: 0.1187 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 0.0014\n",
      "Epoch 9/200\n",
      "\u001b[1m4/6\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - completeness_accuracy: 0.3086 - completeness_loss: 0.1390 - completeness_precision: 0.2508 - completeness_recall: 0.0827 - loss: 0.4754 - understanding_accuracy: 0.2982 - understanding_loss: 0.1579 - understanding_precision: 0.2500 - understanding_recall: 0.1510\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0011199999717064202.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - completeness_accuracy: 0.3227 - completeness_loss: 0.1338 - completeness_precision: 0.2544 - completeness_recall: 0.0843 - loss: 0.4821 - understanding_accuracy: 0.3141 - understanding_loss: 0.1649 - understanding_precision: 0.2793 - understanding_recall: 0.1860 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1091 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4085 - val_understanding_accuracy: 0.3103 - val_understanding_loss: 0.1213 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 0.0011\n",
      "Epoch 10/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - completeness_accuracy: 0.3272 - completeness_loss: 0.1222 - completeness_precision: 0.3446 - completeness_recall: 0.0865 - loss: 0.4308 - understanding_accuracy: 0.3296 - understanding_loss: 0.1325 - understanding_precision: 0.3310 - understanding_recall: 0.1378 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1078 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4008 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1156 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 0.0018\n",
      "Epoch 11/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - completeness_accuracy: 0.3907 - completeness_loss: 0.1184 - completeness_precision: 0.3581 - completeness_recall: 0.0726 - loss: 0.4272 - understanding_accuracy: 0.2994 - understanding_loss: 0.1286 - understanding_precision: 0.3134 - understanding_recall: 0.0942 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1072 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4034 - val_understanding_accuracy: 0.3103 - val_understanding_loss: 0.1192 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 0.0014\n",
      "Epoch 12/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - completeness_accuracy: 0.3411 - completeness_loss: 0.1258 - completeness_precision: 0.3868 - completeness_recall: 0.0816 - loss: 0.4372 - understanding_accuracy: 0.3499 - understanding_loss: 0.1349 - understanding_precision: 0.4006 - understanding_recall: 0.1730 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1116 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4051 - val_understanding_accuracy: 0.3103 - val_understanding_loss: 0.1172 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 0.0014\n",
      "Epoch 13/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - completeness_accuracy: 0.3252 - completeness_loss: 0.1244 - completeness_precision: 0.3453 - completeness_recall: 0.1270 - loss: 0.4155 - understanding_accuracy: 0.4585 - understanding_loss: 0.1143 - understanding_precision: 0.5656 - understanding_recall: 0.1039 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1084 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3979 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1138 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 0.0014\n",
      "Epoch 14/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - completeness_accuracy: 0.2914 - completeness_loss: 0.1214 - completeness_precision: 0.3095 - completeness_recall: 0.0674 - loss: 0.4177 - understanding_accuracy: 0.3193 - understanding_loss: 0.1184 - understanding_precision: 0.2763 - understanding_recall: 0.0413 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1259 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.4090 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1080 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 0.0014\n",
      "Epoch 15/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - completeness_accuracy: 0.3395 - completeness_loss: 0.1301 - completeness_precision: 0.2854 - completeness_recall: 0.0545 - loss: 0.4247 - understanding_accuracy: 0.3249 - understanding_loss: 0.1198 - understanding_precision: 0.1790 - understanding_recall: 0.0224 - val_completeness_accuracy: 0.3103 - val_completeness_loss: 0.1078 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3892 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.8000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - completeness_accuracy: 0.3362 - completeness_loss: 0.1117 - completeness_precision: 0.5973 - completeness_recall: 0.0408 - loss: 0.4034 - understanding_accuracy: 0.3864 - understanding_loss: 0.1182 - understanding_precision: 0.2527 - understanding_recall: 0.0258 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1109 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3936 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1087 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.8000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - completeness_accuracy: 0.3399 - completeness_loss: 0.1133 - completeness_precision: 0.4428 - completeness_recall: 0.0494 - loss: 0.4056 - understanding_accuracy: 0.2830 - understanding_loss: 0.1177 - understanding_precision: 0.2126 - understanding_recall: 0.0278 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1069 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3923 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1118 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.8000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - completeness_accuracy: 0.3658 - completeness_loss: 0.1173 - completeness_precision: 0.5595 - completeness_recall: 0.0225 - loss: 0.4110 - understanding_accuracy: 0.2896 - understanding_loss: 0.1197 - understanding_precision: 0.3116 - understanding_recall: 0.0293 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1102 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3907 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1074 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.8000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - completeness_accuracy: 0.3209 - completeness_loss: 0.1214 - completeness_precision: 0.1889 - completeness_recall: 0.0150 - loss: 0.4094 - understanding_accuracy: 0.3030 - understanding_loss: 0.1181 - understanding_precision: 0.2296 - understanding_recall: 0.0101 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1071 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3888 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1092 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.8000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - completeness_accuracy: 0.3036 - completeness_loss: 0.1152 - completeness_precision: 0.5599 - completeness_recall: 0.0407 - loss: 0.4071 - understanding_accuracy: 0.3131 - understanding_loss: 0.1199 - understanding_precision: 0.3657 - understanding_recall: 0.0389 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1077 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3864 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.8000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - completeness_accuracy: 0.2888 - completeness_loss: 0.1209 - completeness_precision: 0.2381 - completeness_recall: 0.0119 - loss: 0.4168 - understanding_accuracy: 0.2858 - understanding_loss: 0.1258 - understanding_precision: 0.1894 - understanding_recall: 0.0131 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1078 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3861 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.8000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - completeness_accuracy: 0.3178 - completeness_loss: 0.1172 - completeness_precision: 0.4456 - completeness_recall: 0.0232 - loss: 0.3983 - understanding_accuracy: 0.3743 - understanding_loss: 0.1084 - understanding_precision: 0.2714 - understanding_recall: 0.0093 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1068 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3875 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1096 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.8600e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - completeness_accuracy: 0.3491 - completeness_loss: 0.1158 - completeness_precision: 0.5238 - completeness_recall: 0.0157 - loss: 0.4081 - understanding_accuracy: 0.2961 - understanding_loss: 0.1259 - understanding_precision: 0.2124 - understanding_recall: 0.0143 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1080 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3873 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1086 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.8600e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - completeness_accuracy: 0.2757 - completeness_loss: 0.1131 - completeness_precision: 0.6071 - completeness_recall: 0.0180 - loss: 0.3984 - understanding_accuracy: 0.3623 - understanding_loss: 0.1126 - understanding_precision: 0.6735 - understanding_recall: 0.0210 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1088 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3862 - val_understanding_accuracy: 0.3103 - val_understanding_loss: 0.1071 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.8600e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - completeness_accuracy: 0.2951 - completeness_loss: 0.1122 - completeness_precision: 0.4190 - completeness_recall: 0.0075 - loss: 0.4000 - understanding_accuracy: 0.2758 - understanding_loss: 0.1151 - understanding_precision: 0.3190 - understanding_recall: 0.0150 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1102 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3884 - val_understanding_accuracy: 0.3103 - val_understanding_loss: 0.1082 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.8600e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - completeness_accuracy: 0.3084 - completeness_loss: 0.1086 - completeness_precision: 0.5048 - completeness_recall: 0.0172 - loss: 0.3812 - understanding_accuracy: 0.4390 - understanding_loss: 0.1046 - understanding_precision: 0.6155 - understanding_recall: 0.0494 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1070 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3861 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1096 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.8600e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - completeness_accuracy: 0.2614 - completeness_loss: 0.1163 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.4099 - understanding_accuracy: 0.2401 - understanding_loss: 0.1241 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1075 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3880 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1114 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.8600e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - completeness_accuracy: 0.3795 - completeness_loss: 0.1139 - completeness_precision: 0.1000 - completeness_recall: 0.0028 - loss: 0.3983 - understanding_accuracy: 0.4142 - understanding_loss: 0.1153 - understanding_precision: 0.4556 - understanding_recall: 0.0192        \n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0004801999893970787.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - completeness_accuracy: 0.3745 - completeness_loss: 0.1145 - completeness_precision: 0.1429 - completeness_recall: 0.0038 - loss: 0.3998 - understanding_accuracy: 0.3993 - understanding_loss: 0.1167 - understanding_precision: 0.3889 - understanding_recall: 0.0172 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1094 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3867 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1086 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.8020e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - completeness_accuracy: 0.3105 - completeness_loss: 0.1123 - completeness_precision: 0.2905 - completeness_recall: 0.0143 - loss: 0.3910 - understanding_accuracy: 0.3480 - understanding_loss: 0.1079 - understanding_precision: 0.6250 - understanding_recall: 0.0247 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1101 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3851 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1066 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.8020e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - completeness_accuracy: 0.3995 - completeness_loss: 0.1140 - completeness_precision: 0.4102 - completeness_recall: 0.0308 - loss: 0.3910 - understanding_accuracy: 0.3429 - understanding_loss: 0.1084 - understanding_precision: 0.3122 - understanding_recall: 0.0117 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1072 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3823 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1070 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.8020e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - completeness_accuracy: 0.3508 - completeness_loss: 0.1113 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3991 - understanding_accuracy: 0.3227 - understanding_loss: 0.1189 - understanding_precision: 0.1922 - understanding_recall: 0.0075 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1086 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3832 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.8020e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - completeness_accuracy: 0.3241 - completeness_loss: 0.1158 - completeness_precision: 0.4286 - completeness_recall: 0.0026 - loss: 0.3856 - understanding_accuracy: 0.4817 - understanding_loss: 0.1039 - understanding_precision: 0.8000 - understanding_recall: 0.0299 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1114 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3856 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.8020e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - completeness_accuracy: 0.4150 - completeness_loss: 0.1094 - completeness_precision: 0.1429 - completeness_recall: 0.0038 - loss: 0.3922 - understanding_accuracy: 0.2717 - understanding_loss: 0.1136 - understanding_precision: 0.1833 - understanding_recall: 0.0075 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1068 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3813 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1074 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.8020e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - completeness_accuracy: 0.3256 - completeness_loss: 0.1129 - completeness_precision: 0.2857 - completeness_recall: 0.0038 - loss: 0.3893 - understanding_accuracy: 0.3432 - understanding_loss: 0.1092 - understanding_precision: 0.1667 - understanding_recall: 0.0079 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1080 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3821 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1072 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.8020e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - completeness_accuracy: 0.3624 - completeness_loss: 0.1139 - completeness_precision: 0.3714 - completeness_recall: 0.0119 - loss: 0.3890 - understanding_accuracy: 0.4007 - understanding_loss: 0.1113 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1085 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3819 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.8020e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - completeness_accuracy: 0.3462 - completeness_loss: 0.1085 - completeness_precision: 0.7524 - completeness_recall: 0.0236 - loss: 0.3947 - understanding_accuracy: 0.3162 - understanding_loss: 0.1186 - understanding_precision: 0.6667 - understanding_recall: 0.0157 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1077 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3814 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1074 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.3614e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - completeness_accuracy: 0.3891 - completeness_loss: 0.1097 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3901 - understanding_accuracy: 0.3197 - understanding_loss: 0.1130 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1072 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3823 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1090 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.3614e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - completeness_accuracy: 0.3643 - completeness_loss: 0.1061 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3933 - understanding_accuracy: 0.3580 - understanding_loss: 0.1209 - understanding_precision: 0.2667 - understanding_recall: 0.0119 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1070 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3815 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1085 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.3614e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - completeness_accuracy: 0.3337 - completeness_loss: 0.1107 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3917 - understanding_accuracy: 0.3335 - understanding_loss: 0.1139 - understanding_precision: 0.3660 - understanding_recall: 0.0214 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1081 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3813 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1075 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.3614e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - completeness_accuracy: 0.3325 - completeness_loss: 0.1076 - completeness_precision: 0.0667 - completeness_recall: 0.0012 - loss: 0.3855 - understanding_accuracy: 0.3771 - understanding_loss: 0.1122 - understanding_precision: 0.0500 - understanding_recall: 0.0012                \n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0002352979907300323.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - completeness_accuracy: 0.3286 - completeness_loss: 0.1075 - completeness_precision: 0.1429 - completeness_recall: 0.0026 - loss: 0.3860 - understanding_accuracy: 0.3658 - understanding_loss: 0.1147 - understanding_precision: 0.1071 - understanding_recall: 0.0026 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1095 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3822 - val_understanding_accuracy: 0.3103 - val_understanding_loss: 0.1071 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.3530e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - completeness_accuracy: 0.3413 - completeness_loss: 0.1122 - completeness_precision: 0.4286 - completeness_recall: 0.0052 - loss: 0.3897 - understanding_accuracy: 0.3247 - understanding_loss: 0.1117 - understanding_precision: 0.3571 - understanding_recall: 0.0070 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1083 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3804 - val_understanding_accuracy: 0.3103 - val_understanding_loss: 0.1069 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.3614e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - completeness_accuracy: 0.2969 - completeness_loss: 0.1132 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3946 - understanding_accuracy: 0.3415 - understanding_loss: 0.1163 - understanding_precision: 0.1905 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1072 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3796 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1074 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.3614e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - completeness_accuracy: 0.2922 - completeness_loss: 0.1155 - completeness_precision: 0.3810 - completeness_recall: 0.0119 - loss: 0.3925 - understanding_accuracy: 0.3933 - understanding_loss: 0.1100 - understanding_precision: 0.0857 - understanding_recall: 0.0026 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1075 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3812 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1089 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.3530e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - completeness_accuracy: 0.3699 - completeness_loss: 0.1122 - completeness_precision: 0.3571 - completeness_recall: 0.0038 - loss: 0.3884 - understanding_accuracy: 0.3024 - understanding_loss: 0.1117 - understanding_precision: 0.4857 - understanding_recall: 0.0184 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1076 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3806 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1083 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.3530e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - completeness_accuracy: 0.3726 - completeness_loss: 0.1141 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3885 - understanding_accuracy: 0.3057 - understanding_loss: 0.1117 - understanding_precision: 0.3267 - understanding_recall: 0.0165 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1077 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3798 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1076 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.3530e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - completeness_accuracy: 0.3191 - completeness_loss: 0.1124 - completeness_precision: 1.0000 - completeness_recall: 0.0119 - loss: 0.3936 - understanding_accuracy: 0.2780 - understanding_loss: 0.1194 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1080 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3802 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1079 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.3530e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - completeness_accuracy: 0.3458 - completeness_loss: 0.1090 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3886 - understanding_accuracy: 0.2437 - understanding_loss: 0.1144 - understanding_precision: 0.3762 - understanding_recall: 0.0172 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1075 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3796 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1079 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.3530e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - completeness_accuracy: 0.3445 - completeness_loss: 0.1101 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3929 - understanding_accuracy: 0.2623 - understanding_loss: 0.1179 - understanding_precision: 0.6429 - understanding_recall: 0.0194 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1068 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3785 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1076 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.3530e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - completeness_accuracy: 0.3391 - completeness_loss: 0.1127 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3937 - understanding_accuracy: 0.3377 - understanding_loss: 0.1158 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3776 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1071 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.3530e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - completeness_accuracy: 0.3323 - completeness_loss: 0.1090 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3801 - understanding_accuracy: 0.3920 - understanding_loss: 0.1064 - understanding_precision: 0.2143 - understanding_recall: 0.0026 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1068 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3775 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1070 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.6471e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - completeness_accuracy: 0.3213 - completeness_loss: 0.1117 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3895 - understanding_accuracy: 0.3258 - understanding_loss: 0.1156 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1073 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3779 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1070 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.6471e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - completeness_accuracy: 0.3235 - completeness_loss: 0.1120 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3911 - understanding_accuracy: 0.2408 - understanding_loss: 0.1159 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1074 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3784 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1075 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.6471e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - completeness_accuracy: 0.3782 - completeness_loss: 0.1088 - completeness_precision: 0.2976 - completeness_recall: 0.0119 - loss: 0.3864 - understanding_accuracy: 0.2927 - understanding_loss: 0.1133 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1074 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3784 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1076 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.6471e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - completeness_accuracy: 0.3335 - completeness_loss: 0.1108 - completeness_precision: 0.6667 - completeness_recall: 0.0157 - loss: 0.3866 - understanding_accuracy: 0.3444 - understanding_loss: 0.1113 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1073 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3782 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1076 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.6471e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - completeness_accuracy: 0.3688 - completeness_loss: 0.1065 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3791 - understanding_accuracy: 0.3435 - understanding_loss: 0.1102 - understanding_precision: 0.4286 - understanding_recall: 0.0052 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1072 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3777 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1073 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.6471e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - completeness_accuracy: 0.3041 - completeness_loss: 0.1133 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3866 - understanding_accuracy: 0.3752 - understanding_loss: 0.1101 - understanding_precision: 0.3571 - understanding_recall: 0.0052 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1069 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3769 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1069 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.6471e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - completeness_accuracy: 0.4440 - completeness_loss: 0.1056 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3807 - understanding_accuracy: 0.3207 - understanding_loss: 0.1095 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3765 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1069 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.1530e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - completeness_accuracy: 0.3964 - completeness_loss: 0.1080 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3857 - understanding_accuracy: 0.3374 - understanding_loss: 0.1159 - understanding_precision: 0.1714 - understanding_recall: 0.0053 - val_completeness_accuracy: 0.2759 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3763 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.1530e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - completeness_accuracy: 0.4134 - completeness_loss: 0.1050 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3750 - understanding_accuracy: 0.3713 - understanding_loss: 0.1066 - understanding_precision: 0.1071 - understanding_recall: 0.0026 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1069 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3765 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.1530e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - completeness_accuracy: 0.3654 - completeness_loss: 0.1054 - completeness_precision: 1.0000 - completeness_recall: 0.0119 - loss: 0.3751 - understanding_accuracy: 0.3801 - understanding_loss: 0.1087 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1072 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3768 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1069 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.1530e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - completeness_accuracy: 0.3607 - completeness_loss: 0.1080 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3814 - understanding_accuracy: 0.3627 - understanding_loss: 0.1111 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1072 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3764 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.1530e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - completeness_accuracy: 0.3327 - completeness_loss: 0.1080 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3875 - understanding_accuracy: 0.2985 - understanding_loss: 0.1170 - understanding_precision: 0.8571 - understanding_recall: 0.0075 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1069 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3760 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1066 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.1530e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - completeness_accuracy: 0.2653 - completeness_loss: 0.1137 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3861 - understanding_accuracy: 0.3508 - understanding_loss: 0.1097 - understanding_precision: 0.7143 - understanding_recall: 0.0101 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1068 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3760 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.1530e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - completeness_accuracy: 0.2829 - completeness_loss: 0.1151 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3891 - understanding_accuracy: 0.3522 - understanding_loss: 0.1116 - understanding_precision: 0.5714 - understanding_recall: 0.0119 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3759 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 8.0707e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - completeness_accuracy: 0.3603 - completeness_loss: 0.1089 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3854 - understanding_accuracy: 0.2892 - understanding_loss: 0.1124 - understanding_precision: 0.5714 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3758 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1069 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 8.0707e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - completeness_accuracy: 0.3783 - completeness_loss: 0.1044 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3792 - understanding_accuracy: 0.3232 - understanding_loss: 0.1111 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3758 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1069 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 8.0707e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - completeness_accuracy: 0.3222 - completeness_loss: 0.1107 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3881 - understanding_accuracy: 0.3654 - understanding_loss: 0.1153 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3756 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 8.0707e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - completeness_accuracy: 0.4192 - completeness_loss: 0.1044 - completeness_precision: 0.8571 - completeness_recall: 0.0145 - loss: 0.3749 - understanding_accuracy: 0.3984 - understanding_loss: 0.1081 - understanding_precision: 0.2857 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3756 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 8.0707e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - completeness_accuracy: 0.3352 - completeness_loss: 0.1092 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3836 - understanding_accuracy: 0.3702 - understanding_loss: 0.1127 - understanding_precision: 0.2071 - understanding_recall: 0.0079 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3755 - val_understanding_accuracy: 0.4138 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 8.0707e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - completeness_accuracy: 0.3253 - completeness_loss: 0.1120 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3868 - understanding_accuracy: 0.3091 - understanding_loss: 0.1134 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1068 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3755 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 8.0707e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - completeness_accuracy: 0.3429 - completeness_loss: 0.1103 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3824 - understanding_accuracy: 0.2921 - understanding_loss: 0.1124 - understanding_precision: 0.7857 - understanding_recall: 0.0119 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1068 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3756 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 5.6495e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - completeness_accuracy: 0.3387 - completeness_loss: 0.1078 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3768 - understanding_accuracy: 0.3673 - understanding_loss: 0.1075 - understanding_precision: 0.2143 - understanding_recall: 0.0026 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3755 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1069 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 5.6495e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - completeness_accuracy: 0.3065 - completeness_loss: 0.1089 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3835 - understanding_accuracy: 0.3476 - understanding_loss: 0.1131 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3754 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1069 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 5.6495e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - completeness_accuracy: 0.3797 - completeness_loss: 0.1102 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3795 - understanding_accuracy: 0.3919 - understanding_loss: 0.1083 - understanding_precision: 0.1429 - understanding_recall: 0.0026 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3753 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1069 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 5.6495e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - completeness_accuracy: 0.3160 - completeness_loss: 0.1086 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3804 - understanding_accuracy: 0.3443 - understanding_loss: 0.1102 - understanding_precision: 0.8571 - understanding_recall: 0.0101 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3752 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 5.6495e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - completeness_accuracy: 0.3099 - completeness_loss: 0.1116 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3848 - understanding_accuracy: 0.2718 - understanding_loss: 0.1116 - understanding_precision: 1.0000 - understanding_recall: 0.0194 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3751 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 5.6495e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - completeness_accuracy: 0.4069 - completeness_loss: 0.1060 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3845 - understanding_accuracy: 0.2683 - understanding_loss: 0.1162 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3751 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 5.6495e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - completeness_accuracy: 0.3871 - completeness_loss: 0.1099 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3892 - understanding_accuracy: 0.2773 - understanding_loss: 0.1169 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1067 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3750 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.9547e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - completeness_accuracy: 0.2838 - completeness_loss: 0.1085 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3850 - understanding_accuracy: 0.3081 - understanding_loss: 0.1156 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3749 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.9547e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - completeness_accuracy: 0.3023 - completeness_loss: 0.1127 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3869 - understanding_accuracy: 0.3641 - understanding_loss: 0.1123 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3748 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.9547e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - completeness_accuracy: 0.2948 - completeness_loss: 0.1090 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3837 - understanding_accuracy: 0.2909 - understanding_loss: 0.1161 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3748 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.9547e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - completeness_accuracy: 0.3034 - completeness_loss: 0.1164 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3895 - understanding_accuracy: 0.3295 - understanding_loss: 0.1119 - understanding_precision: 0.2857 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3748 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.9547e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - completeness_accuracy: 0.3371 - completeness_loss: 0.1104 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3871 - understanding_accuracy: 0.2931 - understanding_loss: 0.1136 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3748 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.9547e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - completeness_accuracy: 0.2693 - completeness_loss: 0.1122 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3852 - understanding_accuracy: 0.2674 - understanding_loss: 0.1123 - understanding_precision: 0.3571 - understanding_recall: 0.0052 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3748 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.9547e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - completeness_accuracy: 0.3230 - completeness_loss: 0.1131 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3842 - understanding_accuracy: 0.3840 - understanding_loss: 0.1087 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3748 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.7683e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - completeness_accuracy: 0.3202 - completeness_loss: 0.1108 - completeness_precision: 1.0000 - completeness_recall: 0.0157 - loss: 0.3849 - understanding_accuracy: 0.3503 - understanding_loss: 0.1122 - understanding_precision: 0.5476 - understanding_recall: 0.0119 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3747 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.7683e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - completeness_accuracy: 0.2912 - completeness_loss: 0.1153 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3855 - understanding_accuracy: 0.3384 - understanding_loss: 0.1091 - understanding_precision: 0.6429 - understanding_recall: 0.0075 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3747 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.7683e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405ms/step - completeness_accuracy: 0.3665 - completeness_loss: 0.1099 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3803 - understanding_accuracy: 0.3856 - understanding_loss: 0.1099 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3747 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.7683e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - completeness_accuracy: 0.3178 - completeness_loss: 0.1110 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3874 - understanding_accuracy: 0.3315 - understanding_loss: 0.1138 - understanding_precision: 1.0000 - understanding_recall: 0.0157 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3746 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.7683e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - completeness_accuracy: 0.3409 - completeness_loss: 0.1099 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3869 - understanding_accuracy: 0.2783 - understanding_loss: 0.1168 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3746 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.7683e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - completeness_accuracy: 0.4020 - completeness_loss: 0.1063 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3828 - understanding_accuracy: 0.3114 - understanding_loss: 0.1142 - understanding_precision: 0.2619 - understanding_recall: 0.0075 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3746 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.7683e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - completeness_accuracy: 0.3283 - completeness_loss: 0.1072 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3811 - understanding_accuracy: 0.3335 - understanding_loss: 0.1143 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3745 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.9378e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - completeness_accuracy: 0.3296 - completeness_loss: 0.1106 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3838 - understanding_accuracy: 0.3298 - understanding_loss: 0.1126 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3745 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.9378e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - completeness_accuracy: 0.2981 - completeness_loss: 0.1170 - completeness_precision: 1.0000 - completeness_recall: 0.0119 - loss: 0.3888 - understanding_accuracy: 0.3431 - understanding_loss: 0.1119 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3745 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.9378e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - completeness_accuracy: 0.3543 - completeness_loss: 0.1079 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3842 - understanding_accuracy: 0.3144 - understanding_loss: 0.1152 - understanding_precision: 0.8571 - understanding_recall: 0.0157 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3745 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.9378e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - completeness_accuracy: 0.3192 - completeness_loss: 0.1122 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3864 - understanding_accuracy: 0.3576 - understanding_loss: 0.1140 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3745 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.9378e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - completeness_accuracy: 0.2584 - completeness_loss: 0.1146 - completeness_precision: 0.5714 - completeness_recall: 0.0119 - loss: 0.3862 - understanding_accuracy: 0.3370 - understanding_loss: 0.1087 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.4138 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3745 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.9378e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - completeness_accuracy: 0.2957 - completeness_loss: 0.1129 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3812 - understanding_accuracy: 0.3946 - understanding_loss: 0.1081 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3745 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.9378e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - completeness_accuracy: 0.3440 - completeness_loss: 0.1128 - completeness_precision: 0.7143 - completeness_recall: 0.0119 - loss: 0.3874 - understanding_accuracy: 0.2513 - understanding_loss: 0.1147 - understanding_precision: 0.3095 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3744 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.3564e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - completeness_accuracy: 0.3758 - completeness_loss: 0.1120 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3907 - understanding_accuracy: 0.2908 - understanding_loss: 0.1167 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3744 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.3564e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - completeness_accuracy: 0.3252 - completeness_loss: 0.1108 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3788 - understanding_accuracy: 0.3623 - understanding_loss: 0.1083 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3744 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1068 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.3564e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - completeness_accuracy: 0.3420 - completeness_loss: 0.1120 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3818 - understanding_accuracy: 0.3611 - understanding_loss: 0.1086 - understanding_precision: 0.5714 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3744 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.3564e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - completeness_accuracy: 0.3583 - completeness_loss: 0.1088 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3751 - understanding_accuracy: 0.3724 - understanding_loss: 0.1057 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3743 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.3564e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - completeness_accuracy: 0.3621 - completeness_loss: 0.1058 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3791 - understanding_accuracy: 0.3327 - understanding_loss: 0.1115 - understanding_precision: 0.3571 - understanding_recall: 0.0052 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3743 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.3564e-05\n",
      "Epoch 105/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391ms/step - completeness_accuracy: 0.3374 - completeness_loss: 0.1072 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3789 - understanding_accuracy: 0.3777 - understanding_loss: 0.1124 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3743 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.3564e-05\n",
      "Epoch 106/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - completeness_accuracy: 0.3670 - completeness_loss: 0.1092 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3800 - understanding_accuracy: 0.3218 - understanding_loss: 0.1089 - understanding_precision: 0.5714 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3743 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.4951e-06\n",
      "Epoch 107/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - completeness_accuracy: 0.3081 - completeness_loss: 0.1094 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3837 - understanding_accuracy: 0.2900 - understanding_loss: 0.1139 - understanding_precision: 0.7143 - understanding_recall: 0.0052 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3743 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.4951e-06\n",
      "Epoch 108/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - completeness_accuracy: 0.3116 - completeness_loss: 0.1074 - completeness_precision: 0.7143 - completeness_recall: 0.0052 - loss: 0.3768 - understanding_accuracy: 0.4026 - understanding_loss: 0.1083 - understanding_precision: 0.1429 - understanding_recall: 0.0018 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3743 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.4951e-06\n",
      "Epoch 109/200\n",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - completeness_accuracy: 0.2630 - completeness_loss: 0.1120 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3830 - understanding_accuracy: 0.3699 - understanding_loss: 0.1100 - understanding_precision: 0.7333 - understanding_recall: 0.0129      \n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 6.646586007263977e-06.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - completeness_accuracy: 0.2703 - completeness_loss: 0.1105 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3823 - understanding_accuracy: 0.3676 - understanding_loss: 0.1092 - understanding_precision: 0.7143 - understanding_recall: 0.0127 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3743 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.6466e-06\n",
      "Epoch 110/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411ms/step - completeness_accuracy: 0.3751 - completeness_loss: 0.1055 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3804 - understanding_accuracy: 0.3632 - understanding_loss: 0.1128 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3743 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.4951e-06\n",
      "Epoch 111/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - completeness_accuracy: 0.3679 - completeness_loss: 0.1082 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3788 - understanding_accuracy: 0.3667 - understanding_loss: 0.1110 - understanding_precision: 0.1905 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3742 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.4951e-06\n",
      "Epoch 112/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - completeness_accuracy: 0.3351 - completeness_loss: 0.1074 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3754 - understanding_accuracy: 0.3970 - understanding_loss: 0.1064 - understanding_precision: 0.5714 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3742 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 9.4951e-06\n",
      "Epoch 113/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - completeness_accuracy: 0.2574 - completeness_loss: 0.1109 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3864 - understanding_accuracy: 0.2892 - understanding_loss: 0.1154 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3742 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.6466e-06\n",
      "Epoch 114/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - completeness_accuracy: 0.2306 - completeness_loss: 0.1149 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3887 - understanding_accuracy: 0.3661 - understanding_loss: 0.1118 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3742 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.6466e-06\n",
      "Epoch 115/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - completeness_accuracy: 0.3274 - completeness_loss: 0.1120 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3802 - understanding_accuracy: 0.3720 - understanding_loss: 0.1074 - understanding_precision: 0.3571 - understanding_recall: 0.0075 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3742 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.6466e-06\n",
      "Epoch 116/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - completeness_accuracy: 0.3441 - completeness_loss: 0.1106 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3832 - understanding_accuracy: 0.2528 - understanding_loss: 0.1134 - understanding_precision: 0.3571 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3742 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.6466e-06\n",
      "Epoch 117/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - completeness_accuracy: 0.2990 - completeness_loss: 0.1126 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3840 - understanding_accuracy: 0.3286 - understanding_loss: 0.1096 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.652610141420154e-06.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - completeness_accuracy: 0.2923 - completeness_loss: 0.1128 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3845 - understanding_accuracy: 0.3272 - understanding_loss: 0.1093 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3742 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.6526e-06\n",
      "Epoch 118/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - completeness_accuracy: 0.2752 - completeness_loss: 0.1135 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3812 - understanding_accuracy: 0.3860 - understanding_loss: 0.1069 - understanding_precision: 0.2143 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.6466e-06\n",
      "Epoch 119/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - completeness_accuracy: 0.3562 - completeness_loss: 0.1120 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3877 - understanding_accuracy: 0.3175 - understanding_loss: 0.1151 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 6.6466e-06\n",
      "Epoch 120/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - completeness_accuracy: 0.3217 - completeness_loss: 0.1115 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3821 - understanding_accuracy: 0.3570 - understanding_loss: 0.1092 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.6526e-06\n",
      "Epoch 121/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - completeness_accuracy: 0.4002 - completeness_loss: 0.1106 - completeness_precision: 0.7143 - completeness_recall: 0.0052 - loss: 0.3808 - understanding_accuracy: 0.3854 - understanding_loss: 0.1088 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.6526e-06\n",
      "Epoch 122/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - completeness_accuracy: 0.3861 - completeness_loss: 0.1064 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3749 - understanding_accuracy: 0.3833 - understanding_loss: 0.1061 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.6526e-06\n",
      "Epoch 123/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - completeness_accuracy: 0.3197 - completeness_loss: 0.1065 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3767 - understanding_accuracy: 0.3805 - understanding_loss: 0.1090 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.6526e-06\n",
      "Epoch 124/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - completeness_accuracy: 0.4110 - completeness_loss: 0.1058 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3813 - understanding_accuracy: 0.2898 - understanding_loss: 0.1144 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.6526e-06\n",
      "Epoch 125/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - completeness_accuracy: 0.3436 - completeness_loss: 0.1078 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3819 - understanding_accuracy: 0.3176 - understanding_loss: 0.1127 - understanding_precision: 0.4000 - understanding_recall: 0.0128      \n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.2568271308264228e-06.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - completeness_accuracy: 0.3445 - completeness_loss: 0.1078 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3823 - understanding_accuracy: 0.3151 - understanding_loss: 0.1127 - understanding_precision: 0.4000 - understanding_recall: 0.0127 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.2568e-06\n",
      "Epoch 126/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - completeness_accuracy: 0.3035 - completeness_loss: 0.1102 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3811 - understanding_accuracy: 0.3508 - understanding_loss: 0.1073 - understanding_precision: 0.1429 - understanding_recall: 0.0018 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 4.6526e-06\n",
      "Epoch 127/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - completeness_accuracy: 0.3729 - completeness_loss: 0.1079 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3782 - understanding_accuracy: 0.3137 - understanding_loss: 0.1090 - understanding_precision: 0.4286 - understanding_recall: 0.0026 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.2568e-06\n",
      "Epoch 128/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - completeness_accuracy: 0.3609 - completeness_loss: 0.1086 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3798 - understanding_accuracy: 0.3490 - understanding_loss: 0.1125 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.2568e-06\n",
      "Epoch 129/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - completeness_accuracy: 0.2319 - completeness_loss: 0.1156 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3952 - understanding_accuracy: 0.2747 - understanding_loss: 0.1173 - understanding_precision: 0.5000 - understanding_recall: 0.0052 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.2568e-06\n",
      "Epoch 130/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - completeness_accuracy: 0.3194 - completeness_loss: 0.1105 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3873 - understanding_accuracy: 0.3038 - understanding_loss: 0.1168 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.2568e-06\n",
      "Epoch 131/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 493ms/step - completeness_accuracy: 0.3428 - completeness_loss: 0.1096 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3829 - understanding_accuracy: 0.3840 - understanding_loss: 0.1134 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.2568e-06\n",
      "Epoch 132/200\n",
      "\u001b[1m4/6\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - completeness_accuracy: 0.2448 - completeness_loss: 0.1107 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3861 - understanding_accuracy: 0.3535 - understanding_loss: 0.1145 - understanding_precision: 0.7500 - understanding_recall: 0.0085        \n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 2.279778959746181e-06.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - completeness_accuracy: 0.2572 - completeness_loss: 0.1095 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3850 - understanding_accuracy: 0.3466 - understanding_loss: 0.1133 - understanding_precision: 0.8571 - understanding_recall: 0.0075 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.2798e-06\n",
      "Epoch 133/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - completeness_accuracy: 0.3001 - completeness_loss: 0.1106 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3840 - understanding_accuracy: 0.2647 - understanding_loss: 0.1110 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 3.2568e-06\n",
      "Epoch 134/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - completeness_accuracy: 0.3750 - completeness_loss: 0.1057 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3787 - understanding_accuracy: 0.3159 - understanding_loss: 0.1110 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.2798e-06\n",
      "Epoch 135/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - completeness_accuracy: 0.4100 - completeness_loss: 0.1066 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3775 - understanding_accuracy: 0.3073 - understanding_loss: 0.1083 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.2798e-06\n",
      "Epoch 136/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - completeness_accuracy: 0.3221 - completeness_loss: 0.1122 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3858 - understanding_accuracy: 0.3760 - understanding_loss: 0.1134 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.2798e-06\n",
      "Epoch 137/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - completeness_accuracy: 0.2756 - completeness_loss: 0.1143 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3896 - understanding_accuracy: 0.1717 - understanding_loss: 0.1138 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.2798e-06\n",
      "Epoch 138/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - completeness_accuracy: 0.3346 - completeness_loss: 0.1100 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3791 - understanding_accuracy: 0.3894 - understanding_loss: 0.1073 - understanding_precision: 0.7024 - understanding_recall: 0.0184 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.2798e-06\n",
      "Epoch 139/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - completeness_accuracy: 0.3405 - completeness_loss: 0.1125 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3840 - understanding_accuracy: 0.3755 - understanding_loss: 0.1120 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 1.5958452877384843e-06.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - completeness_accuracy: 0.3401 - completeness_loss: 0.1130 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3837 - understanding_accuracy: 0.3753 - understanding_loss: 0.1121 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.5958e-06\n",
      "Epoch 140/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - completeness_accuracy: 0.3711 - completeness_loss: 0.1106 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3825 - understanding_accuracy: 0.3313 - understanding_loss: 0.1130 - understanding_precision: 0.3571 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 2.2798e-06\n",
      "Epoch 141/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - completeness_accuracy: 0.3504 - completeness_loss: 0.1113 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3801 - understanding_accuracy: 0.4033 - understanding_loss: 0.1070 - understanding_precision: 0.0000e+00 - understanding_recall: 0.0000e+00 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.5958e-06\n",
      "Epoch 142/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - completeness_accuracy: 0.3526 - completeness_loss: 0.1096 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3769 - understanding_accuracy: 0.4275 - understanding_loss: 0.1083 - understanding_precision: 0.3571 - understanding_recall: 0.0052 - val_completeness_accuracy: 0.3448 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.5958e-06\n",
      "Epoch 143/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - completeness_accuracy: 0.3173 - completeness_loss: 0.1106 - completeness_precision: 0.0000e+00 - completeness_recall: 0.0000e+00 - loss: 0.3842 - understanding_accuracy: 0.3000 - understanding_loss: 0.1119 - understanding_precision: 0.2857 - understanding_recall: 0.0038 - val_completeness_accuracy: 0.3793 - val_completeness_loss: 0.1066 - val_completeness_precision: 0.0000e+00 - val_completeness_recall: 0.0000e+00 - val_loss: 0.3741 - val_understanding_accuracy: 0.3448 - val_understanding_loss: 0.1067 - val_understanding_precision: 0.0000e+00 - val_understanding_recall: 0.0000e+00 - learning_rate: 1.5958e-06\n",
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'saved_model'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 192), dtype=tf.float32, name='text_input'), TensorSpec(shape=(None, 192), dtype=tf.float32, name='explanation_input')]\n",
      "Output Type:\n",
      "  List[TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)]\n",
      "Captures:\n",
      "  2688970744144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972468688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972470032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970742032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970744528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970741072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970741840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970740112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970740688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970739152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970739728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970738192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972469072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972471376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972466192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972470800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972471760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972470416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972466960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688972470224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970737040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970735888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970736464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970735696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970742800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970744912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970742416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970745104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970746064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970746832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970747024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970747600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970747216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970747792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005012560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005013136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970748752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970749520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005012752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005013328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970748944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970749712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005014288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005015056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970750672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970751440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005015632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005013520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970750864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2688970751632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005016400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005017168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005011216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2689005012368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FeynMindModelV3' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity_model_2.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FeynMindModelV3' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save('similarity_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "saved_model_path = \"./{}.h5\".format(int(time.time()))\n",
    "\n",
    "model.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow Decision Forests 1.8.1 is compatible with the following TensorFlow Versions: ['2.15.0']. However, TensorFlow 2.18.0 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n",
      "WARNING:root:Failure to load the inference.so custom c++ tensorflow ops. This error is likely caused the version of TensorFlow and TensorFlow Decision Forests are not compatible. Full error:c:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "c:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfjs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Validate model path\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflowjs\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-imports\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m converters\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflowjs\\converters\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-imports,line-too-long\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_h5_conversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_keras_model\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tfjs_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_model\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflowjs\\converters\\converter.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_h5_conversion \u001b[38;5;28;01mas\u001b[39;00m conversion\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_tfjs_loader\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_saved_model_conversion_v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZipFile, is_zipfile\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_keras_h5_to_tfjs_layers_model_conversion\u001b[39m(\n\u001b[0;32m     43\u001b[0m     h5_path, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, quantization_dtype_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m     split_weights_by_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m     weight_shard_size_bytes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     46\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflowjs\\converters\\tf_saved_model_conversion_v2.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZipFile\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Required to load saved models that use TFDF.\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\__init__.py:64\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_version\n\u001b[0;32m     62\u001b[0m check_version\u001b[38;5;241m.\u001b[39mcheck_version(__version__, compatible_tf_versions)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_tree\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\__init__.py:53\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Decision Forest in a Keras Model.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mUsage example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrappers\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Utility classes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\core.py:62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tuner \u001b[38;5;28;01mas\u001b[39;00m tuner_lib\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_inference\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cc_logging\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core \u001b[38;5;28;01mas\u001b[39;00m tf_core\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\core_inference.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_inference \u001b[38;5;28;01mas\u001b[39;00m tf_core\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m tf_op\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstract_learner_pb2\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultitasker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multitasker_pb2\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\api.py:179\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf1_compatibility\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_spec_pb2\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstract_model_pb2\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021 Google LLC.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop_dynamic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Importing all the symbols.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m   ops \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_op_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path_to_datafile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference.so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\load_library.py:54\u001b[0m, in \u001b[0;36mload_op_library\u001b[1;34m(library_filename)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_op_library\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_op_library\u001b[39m(library_filename):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a TensorFlow plugin, containing custom ops and kernels.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m  Pass \"library_filename\" to a platform-specific mechanism for dynamically\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RuntimeError: when unable to load the library or get the python wrappers.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m   lib_handle \u001b[38;5;241m=\u001b[39m \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibrary_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     wrappers \u001b[38;5;241m=\u001b[39m _pywrap_python_op_gen\u001b[38;5;241m.\u001b[39mGetPythonWrappers(\n\u001b[0;32m     57\u001b[0m         py_tf\u001b[38;5;241m.\u001b[39mTF_GetOpList(lib_handle))\n",
      "\u001b[1;31mNotFoundError\u001b[0m: c:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflowjs as tfjs\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    # Validate model path\n",
    "    model_path = os.path.abspath('C:/Users/Salsa/PycharmProjects/pytorch/similarity_model.h5')\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "        \n",
    "    output_dir = './web_model'\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load and verify model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Convert model using tfjs API\n",
    "    tfjs.converters.save_keras_model(model, output_dir)\n",
    "    print(f\"Model converted successfully to {os.path.abspath(output_dir)}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Conversion failed: {str(e)}\")\n",
    "finally:\n",
    "    # Cleanup any temporary files if needed\n",
    "    print(\"Conversion process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow Decision Forests 1.8.1 is compatible with the following TensorFlow Versions: ['2.15.0']. However, TensorFlow 2.18.0 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n",
      "WARNING:root:Failure to load the inference.so custom c++ tensorflow ops. This error is likely caused the version of TensorFlow and TensorFlow Decision Forests are not compatible. Full error:c:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "c:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfjs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflowjs_converter --input_format=keras \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mC:/Users/Salsa/PycharmProjects/pytorch/similarity_model.h5} ./\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflowjs\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-imports\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m converters\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflowjs\\converters\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-imports,line-too-long\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_h5_conversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_keras_model\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tfjs_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_model\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflowjs\\converters\\converter.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_h5_conversion \u001b[38;5;28;01mas\u001b[39;00m conversion\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_tfjs_loader\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_saved_model_conversion_v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZipFile, is_zipfile\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_keras_h5_to_tfjs_layers_model_conversion\u001b[39m(\n\u001b[0;32m     43\u001b[0m     h5_path, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, quantization_dtype_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m     split_weights_by_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m     weight_shard_size_bytes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     46\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflowjs\\converters\\tf_saved_model_conversion_v2.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZipFile\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Required to load saved models that use TFDF.\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\__init__.py:64\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_version\n\u001b[0;32m     62\u001b[0m check_version\u001b[38;5;241m.\u001b[39mcheck_version(__version__, compatible_tf_versions)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_tree\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\__init__.py:53\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Decision Forest in a Keras Model.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mUsage example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrappers\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Utility classes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\core.py:62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tuner \u001b[38;5;28;01mas\u001b[39;00m tuner_lib\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_inference\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cc_logging\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core \u001b[38;5;28;01mas\u001b[39;00m tf_core\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\core_inference.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_inference \u001b[38;5;28;01mas\u001b[39;00m tf_core\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m tf_op\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstract_learner_pb2\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultitasker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multitasker_pb2\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\api.py:179\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf1_compatibility\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_spec_pb2\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstract_model_pb2\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021 Google LLC.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop_dynamic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Importing all the symbols.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m   ops \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_op_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path_to_datafile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference.so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\load_library.py:54\u001b[0m, in \u001b[0;36mload_op_library\u001b[1;34m(library_filename)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_op_library\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_op_library\u001b[39m(library_filename):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a TensorFlow plugin, containing custom ops and kernels.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m  Pass \"library_filename\" to a platform-specific mechanism for dynamically\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RuntimeError: when unable to load the library or get the python wrappers.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m   lib_handle \u001b[38;5;241m=\u001b[39m \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibrary_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     wrappers \u001b[38;5;241m=\u001b[39m _pywrap_python_op_gen\u001b[38;5;241m.\u001b[39mGetPythonWrappers(\n\u001b[0;32m     57\u001b[0m         py_tf\u001b[38;5;241m.\u001b[39mTF_GetOpList(lib_handle))\n",
      "\u001b[1;31mNotFoundError\u001b[0m: c:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found"
     ]
    }
   ],
   "source": [
    "import tensorflowjs as tfjs\n",
    "\n",
    "!tensorflowjs_converter --input_format=keras {C:/Users/Salsa/PycharmProjects/pytorch/similarity_model.h5} ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class QuestionEvaluator:\n",
    "    def __init__(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "    \n",
    "    def prepare_input_data(self, student_explanation: str, reference_pdf_path: str) -> Dict[str, np.ndarray]:\n",
    "        # Clean and process texts\n",
    "        cleaned_explanation = self.processor.clean_text(student_explanation)\n",
    "        reference_text = self.processor.extract_from_pdf(reference_pdf_path)\n",
    "        cleaned_reference = self.processor.clean_text(reference_text)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        explanation_emb = self.processor.generate_embeddings(cleaned_explanation)\n",
    "        reference_emb = self.processor.generate_embeddings(cleaned_reference)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        explanation_emb_np = explanation_emb.cpu().numpy() if torch.cuda.is_available() else explanation_emb.numpy()\n",
    "        reference_emb_np = reference_emb.cpu().numpy() if torch.cuda.is_available() else reference_emb.numpy()\n",
    "        \n",
    "        # Ensure correct shape for model input\n",
    "        if len(explanation_emb_np.shape) == 1:\n",
    "            explanation_emb_np = np.expand_dims(explanation_emb_np, 0)\n",
    "        if len(reference_emb_np.shape) == 1:\n",
    "            reference_emb_np = np.expand_dims(reference_emb_np, 0)\n",
    "            \n",
    "        # Split embeddings\n",
    "        half_dim = explanation_emb_np.shape[1] // 2\n",
    "        return {\n",
    "            \"text_input\": reference_emb_np[:, :half_dim],\n",
    "            \"explanation_input\": explanation_emb_np[:, half_dim:]\n",
    "        }\n",
    "    \n",
    "    def evaluate_explanation(self, student_explanation: str, reference_pdf_path: str) -> Dict[str, Dict[str, float]]:\n",
    "        input_data = self.prepare_input_data(student_explanation, reference_pdf_path)\n",
    "        \n",
    "        # Get predictions using model's predict method\n",
    "        predictions = self.model.model.predict([\n",
    "            input_data['text_input'],\n",
    "            input_data['explanation_input']\n",
    "        ])\n",
    "        \n",
    "        # Convert predictions to probabilities\n",
    "        understanding_pred = predictions[0]\n",
    "        completeness_pred = predictions[1]\n",
    "        \n",
    "        # Format results with confidence scores\n",
    "        results = {\n",
    "            \"understanding\": {\n",
    "                \"Low\": float(understanding_pred[0][0]),\n",
    "                \"Medium\": float(understanding_pred[0][1]), \n",
    "                \"High\": float(understanding_pred[0][2])\n",
    "            },\n",
    "            \"completeness\": {\n",
    "                \"Incomplete\": float(completeness_pred[0][0]),\n",
    "                \"Partial\": float(completeness_pred[0][1]),\n",
    "                \"Complete\": float(completeness_pred[0][2])\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"understanding_confidence\": float(np.max(understanding_pred)),\n",
    "                \"completeness_confidence\": float(np.max(completeness_pred))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def bulk_evaluate(self, explanations_dir: str, references_dir: str) -> List[Dict]:\n",
    "        evaluation_results = []\n",
    "        \n",
    "        txt_files = [f for f in os.listdir(explanations_dir) if f.endswith('.txt')]\n",
    "        pdf_files = [f for f in os.listdir(references_dir) if f.endswith('.pdf')]\n",
    "        \n",
    "        for txt_file in txt_files:\n",
    "            base_name = os.path.splitext(txt_file)[0]\n",
    "            matching_pdf = f\"{base_name}.pdf\"  # Changed from .txt to .pdf\n",
    "            \n",
    "            if matching_pdf in pdf_files:\n",
    "                txt_path = os.path.join(explanations_dir, txt_file)\n",
    "                pdf_path = os.path.join(references_dir, matching_pdf)\n",
    "                \n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    student_explanation = f.read()\n",
    "                \n",
    "                try:\n",
    "                    result = self.evaluate_explanation(student_explanation, pdf_path)\n",
    "                    result['filename'] = txt_file\n",
    "                    evaluation_results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {txt_file}: {e}\")\n",
    "        \n",
    "        return evaluation_results\n",
    "\n",
    "def visualize_evaluation_results(results: List[Dict]):\n",
    "    \"\"\"Enhanced visualization with confidence metrics\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Understanding scores\n",
    "    plt.subplot(1, 3, 1)\n",
    "    understanding_scores = [r['understanding']['High'] for r in results]\n",
    "    plt.hist(understanding_scores, bins=10, color='blue', alpha=0.7)\n",
    "    plt.title('Understanding Scores')\n",
    "    plt.xlabel('High Understanding Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Completeness scores\n",
    "    plt.subplot(1, 3, 2)\n",
    "    completeness_scores = [r['completeness']['Complete'] for r in results]\n",
    "    plt.hist(completeness_scores, bins=10, color='green', alpha=0.7)\n",
    "    plt.title('Completeness Scores')\n",
    "    plt.xlabel('Complete Score Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Confidence metrics\n",
    "    plt.subplot(1, 3, 3)\n",
    "    confidence_u = [r['metrics']['understanding_confidence'] for r in results]\n",
    "    confidence_c = [r['metrics']['completeness_confidence'] for r in results]\n",
    "    plt.boxplot([confidence_u, confidence_c], labels=['Understanding', 'Completeness'])\n",
    "    plt.title('Prediction Confidence')\n",
    "    plt.ylabel('Confidence Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_evaluator(model_path=None):\n",
    "    \"\"\"\n",
    "    Set up the QuestionEvaluator with a trained model\n",
    "    \n",
    "    Args:\n",
    "        model_path (str, optional): Path to saved model weights\n",
    "    \n",
    "    Returns:\n",
    "        QuestionEvaluator instance\n",
    "    \"\"\"\n",
    "    # Initialize processor and model\n",
    "    processor = TextProcessorWithPyMuPDF()\n",
    "    model = FeynMindModelV2()\n",
    "    \n",
    "    # Load pre-trained weights if path provided\n",
    "    if model_path and os.path.exists(model_path):\n",
    "        model.model.load_weights(model_path)\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = QuestionEvaluator(model, processor)\n",
    "    \n",
    "    return evaluator\n",
    "\n",
    "def main_evaluation():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate evaluation process\n",
    "    \"\"\"\n",
    "    # Set up evaluator\n",
    "    evaluator = setup_evaluator()\n",
    "    \n",
    "    # Example usage for single explanation\n",
    "    student_explanation = \"Pada bab *Menjelajah Sel*, terdapat beberapa poin penting yang perlu dipahami. Pada *Aktivitas 1.1*, objek yang tidak dapat diamati dengan mikroskop cahaya meliputi virus T2 fag, protein, lipid, molekul-molekul kecil, dan atom. Sebaliknya, objek yang dapat diamati mencakup telur ikan, burung kolibri, manusia, paus biru, dan pohon pinus raksasa. Pada *Aktivitas 1.2*, saya membaca artikel berjudul *Pemeriksaan Sitologi Aspiratif untuk Mendeteksi Kanker Paru*. Artikel ini menjelaskan bahwa para peneliti menggunakan berbagai metode dalam penelitian sitologi untuk mendukung deteksi kanker paru.  Pada *Aktivitas 1.3*, jenis-jenis mikroskop juga dipelajari. Mikroskop cahaya mampu memperbesar hingga 1.000 kali, sementara mikroskop elektron memiliki kemampuan hingga 1.000.000 kali. Namun, perbesaran maksimal pada mikroskop elektron sering kali sulit tercapai karena berbagai faktor teknis. Di *Aktivitas 1.4*, kita mengenal bagian-bagian mikroskop dan fungsinya. Misalnya, lensa okuler berfungsi memperbesar bayangan dari lensa objektif, tabung menghubungkan kedua lensa tersebut, sementara sekrup pengarah kasar dan halus digunakan untuk mengatur fokus dengan tingkat kecepatan yang berbeda. Ada juga revolver yang digunakan untuk mengganti perbesaran lensa objektif, serta meja benda yang menjadi tempat meletakkan preparat.  Selanjutnya, pada *Aktivitas 1.7*, dibahas enam jenis sel yang mendukung retina mata, yaitu sel fotoreseptor, sel bipolar, sel ganglion retina, sel horizontal, sel amakrin, dan sel pigmen retina. Masing-masing sel memiliki peran khusus dalam memastikan fungsi retina berjalan optimal. Selain itu, penting untuk memahami cara memegang, menyimpan, dan menggunakan mikroskop dengan benar. Mikroskop sebaiknya dipegang dengan tangan kanan pada bagian pegangan, sementara tangan kiri menopang di bawahnya, dan disimpan di tempat yang aman serta kering untuk menjaga keawetannya. Demikian penjelasan mengenai materi ini.\"\n",
    "    reference_pdf_path = \"Perkategori/Biologi-BG-KLS-XI-29-50.pdf\"\n",
    "    \n",
    "    single_result = evaluator.evaluate_explanation(\n",
    "        student_explanation, \n",
    "        reference_pdf_path\n",
    "    )\n",
    "    print(\"Single Explanation Evaluation:\")\n",
    "    print(single_result)\n",
    "    \n",
    "    # Bulk evaluation\n",
    "    bulk_results = evaluator.bulk_evaluate(\n",
    "        explanations_dir=\"Penjelasan/\", \n",
    "        references_dir=\"Perkategori/\"\n",
    "    )\n",
    "    print(\"\\nBulk Evaluation Results:\")\n",
    "    for result in bulk_results:\n",
    "        print(result)\n",
    "\n",
    "# Optional: Add visualization for results\n",
    "def visualize_evaluation_results(results):\n",
    "    \"\"\"\n",
    "    Visualize evaluation results\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Evaluation results from bulk_evaluate\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    understanding_scores = [\n",
    "        result['understanding']['High'] for result in results\n",
    "    ]\n",
    "    completeness_scores = [\n",
    "        result['completeness']['Complete'] for result in results\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(understanding_scores, bins=10, color='blue', alpha=0.7)\n",
    "    plt.title('Distribution of Understanding Scores')\n",
    "    plt.xlabel('High Understanding Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(completeness_scores, bins=10, color='green', alpha=0.7)\n",
    "    plt.title('Distribution of Completeness Scores')\n",
    "    plt.xlabel('Complete Explanation Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Salsa\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['text_input', 'explanation_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step\n",
      "Single Explanation Evaluation:\n",
      "{'understanding': {'Low': 0.3131571114063263, 'Medium': 0.29885348677635193, 'High': 0.3879894018173218}, 'completeness': {'Incomplete': 0.054081209003925323, 'Partial': 0.8973579406738281, 'Complete': 0.04856080934405327}, 'metrics': {'understanding_confidence': 0.3879894018173218, 'completeness_confidence': 0.8973579406738281}}\n",
      "\n",
      "Bulk Evaluation Results:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "visualize_evaluation_results(setup_evaluator().bulk_evaluate(\"Penjelasan/\", \"Perkategori/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-endpoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
